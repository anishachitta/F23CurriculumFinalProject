Anisha Chitta

chitta@usc.edu

Aspects of the code have been derived from USC CAIS++ L4 Curriculum Notebook, but altered to fit the need for this model.

My Project: This project revolves around a model utilizing a Kaggle dataset containing news headlines from TheOnion and HuffPost. Using a pre-trained BERT model, the goal is to enhance it for sarcasm detection. The dataset, represented in JSON format, includes binary sarcasm labels, headlines, and article links.

In the dataset preparation phase, an important step was to split the data into training and testing sets using an 80-20 ratio. The headlines were tokenized using the ‘bert-base-uncased’ pre-trained BERT tokenizer. This tokenizer was selected due to its ability to understand the context of words in sentences and predict masked words, making it highly effective for tasks like text classification and named entity recognition. Tokenization transformed the headlines into numerical representations, which is essential for tasks like sarcasm detection. Special tokens like ‘[CLS]’ and ‘[SEP]’ were added to the beginning and end of each sentence, respectively, to maintain the context. To ensure uniformity, the headlines were either padded or truncated based on the length of the longest sentence in the dataset. The tokenized sequences were then converted into PyTorch tensors to make them compatible with the neural network. The data was randomly split to ensure a balanced representation of different classes, which aids in the model’s ability to generalize effectively.

The sarcasm detection model was built around the 'bert-base-uncased' model. The fine-tuning process utilized an AdamW optimizer with a learning rate of 2e-5 and an epsilon value of 1e-8, following the guidelines provided by Hugging Face. The training was conducted over three epochs, employing techniques such as gradient clipping and a linear learning rate scheduler with warm-up steps to ensure effective learning. Upon evaluation, the model achieved an accuracy of 93.09% on the test set. The training loss decreased from 0.28 to 0.06 across the epochs, indicating the model's improving performance. Although the validation loss slightly increased from 0.18 to 0.32, it remained relatively low, suggesting the model's ability to generalize well. The training and validation times were consistent, approximately 4 minutes and 9 seconds, demonstrating the efficiency of the training process.

The selection of the dataset, model architecture, and training procedures were meticulously done to optimize sarcasm detection. The 'bert-base-uncased' model's ability to understand context, combined with thorough training, resulted in a model that is both reliable and efficient. Future work should include exploring additional specific evaluation metrics and addressing potential biases in the dataset. The model's success in detecting sarcasm suggests potential for wider applications such as content moderation, sentiment analysis, and social media monitoring, thereby promoting more constructive online communication. However, it's important to consider potential biases and limitations in understanding linguistic nuances before applying the model more broadly. As a next step, fine-tuning the model with domain-specific data could potentially improve its performance in specialized areas such as customer service or product reviews. This would allow for more targeted applications of the model in various industries.
